{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5599958506123773\n",
      "epoch= 0\n",
      "Accuracy is= tensor(75.7669)\n",
      "0.5211039007395163\n",
      "epoch= 1\n",
      "Accuracy is= tensor(75.7669)\n",
      "0.5093552065991808\n",
      "epoch= 2\n",
      "Accuracy is= tensor(75.7669)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/sachinprakash/vscode-workspace/aai/ai-project/ai-ducation-analytics/cnn_training.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sachinprakash/vscode-workspace/aai/ai-project/ai-ducation-analytics/cnn_training.ipynb#W2sZmlsZQ%3D%3D?line=120'>121</a>\u001b[0m output \u001b[39m=\u001b[39m model(instances)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sachinprakash/vscode-workspace/aai/ai-project/ai-ducation-analytics/cnn_training.ipynb#W2sZmlsZQ%3D%3D?line=121'>122</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, labels)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/sachinprakash/vscode-workspace/aai/ai-project/ai-ducation-analytics/cnn_training.ipynb#W2sZmlsZQ%3D%3D?line=122'>123</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sachinprakash/vscode-workspace/aai/ai-project/ai-ducation-analytics/cnn_training.ipynb#W2sZmlsZQ%3D%3D?line=123'>124</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sachinprakash/vscode-workspace/aai/ai-project/ai-ducation-analytics/cnn_training.ipynb#W2sZmlsZQ%3D%3D?line=125'>126</a>\u001b[0m running_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39m_execution_engine\u001b[39m.\u001b[39mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torch import optim, nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as td\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "\n",
    "def custom_loader(batch_size, shuffle_test=False, data_path='./dataset/preprocessed/train/'):\n",
    "    # Add the necessary transforms\n",
    "    # normalize = transforms.Normalize(mean=[0.024], std=[0.994])\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((48, 48)),  # Adjust this if your images are a different size\n",
    "        # transforms.Grayscale(num_output_channels=1),  # Convert to grayscale\n",
    "        transforms.ToTensor(),\n",
    "        # normalize\n",
    "    ])\n",
    "\n",
    "    # Load your dataset using ImageFolder\n",
    "    master_dataset = datasets.ImageFolder(root=data_path, transform=transform)\n",
    "\n",
    "    # Calculate the sizes of the splits\n",
    "    total_size = len(master_dataset)\n",
    "    train_size = int(0.85 * total_size)\n",
    "    val_size = total_size - train_size\n",
    "\n",
    "    # Use random_split to create datasets for training, testing, and validation\n",
    "    train_dataset, val_dataset = random_split(master_dataset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader\n",
    "    \n",
    "class MultiLayerFCNet(nn.Module):\n",
    "    def __init__(self,input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer1=nn.Conv2d(3,32,4,padding=1,stride=1)\n",
    "        self.B1 = nn.BatchNorm2d(32)\n",
    "        self.layer2 = nn.Conv2d(32, 32, 4, padding=1, stride=1)\n",
    "        self.B2 = nn.BatchNorm2d(32)\n",
    "        self.Maxpool=nn.MaxPool2d(2)\n",
    "        self.layer3 = nn.Conv2d(32, 64, 4, padding=1, stride=1)\n",
    "        self.B3 = nn.BatchNorm2d(64)\n",
    "        self.layer4 = nn.Conv2d(64, 64, 4, padding=1, stride=1)\n",
    "        self.B4 = nn.BatchNorm2d(64)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        # New layers\n",
    "        self.layer5 = nn.Conv2d(64, 128, 4, padding=1, stride=1)\n",
    "        self.B5 = nn.BatchNorm2d(128)\n",
    "        self.layer6 = nn.Conv2d(128, 128, 4, padding=1, stride=1)\n",
    "        self.B6 = nn.BatchNorm2d(128)\n",
    "        self.layer7 = nn.Conv2d(128, 256, 4, padding=1, stride=1)\n",
    "        self.B7 = nn.BatchNorm2d(256)\n",
    "        self.layer8 = nn.Conv2d(256, 256, 4, padding=1, stride=1)\n",
    "        self.B8 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        # Calculate the size for the fully connected layer after additional max-pooling layers\n",
    "        # Assuming two max-pooling operations in the existing layers\n",
    "        self.fc_size = 256   # Now this is 256 * 3 * 3\n",
    "        self.fc = nn.Linear(self.fc_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through existing layers\n",
    "        x = F.leaky_relu(self.B1(self.layer1(x)))\n",
    "        x = self.Maxpool(F.leaky_relu(self.B2(self.layer2(x))))\n",
    "        x = F.leaky_relu(self.B3(self.layer3(x)))\n",
    "        x = self.Maxpool(F.leaky_relu(self.B4(self.layer4(x))))\n",
    "        \n",
    "        # Pass through new layers\n",
    "        x = F.leaky_relu(self.B5(self.layer5(x)))\n",
    "        x = F.leaky_relu(self.B6(self.layer6(x)))\n",
    "        x = self.Maxpool(F.leaky_relu(self.B7(self.layer7(x))))\n",
    "        x = self.Maxpool(F.leaky_relu(self.B8(self.layer8(x))))\n",
    "        \n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor for the fully connected layer\n",
    "        return self.fc(x)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    batch_size = 64\n",
    "    test_batch_size = 64\n",
    "    input_size = 3 * 48 * 48  # 1 channels, 48x48 image size\n",
    "    hidden_size = 50  # Number of hidden units\n",
    "    output_size = 4  # Number of output classes 4\n",
    "    num_epochs = 10\n",
    "\n",
    "    # train_loader, _ = cifar_loader(batch_size)\n",
    "    # _, test_loader = cifar_loader(test_batch_size)\n",
    "    train_loader, test_loader = custom_loader(batch_size, data_path='./dataset/preprocessed/train/')\n",
    "    # dataloader = DataLoader(dataset=IrisDataset('iris.data'),\n",
    "    #                         batch_size=10,\n",
    "    #                         shuffle=True)\n",
    "\n",
    "    epochs = 50\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = MultiLayerFCNet(input_size, hidden_size, output_size)\n",
    "    model = nn.DataParallel(model)\n",
    "    model.to(device)\n",
    "    #model.load_state_dict(torch.load('path'))\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    BestACC=0.3\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0\n",
    "        for instances, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(instances)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(running_loss / len(train_loader))\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            allsamps=0\n",
    "            rightPred=0\n",
    "\n",
    "            for instances, labels in test_loader:\n",
    "\n",
    "                output = model(instances)\n",
    "                predictedClass=torch.max(output,1)\n",
    "                allsamps+=output.size(0)\n",
    "                rightPred+=(torch.max(output,1)[1]==labels).sum()\n",
    "\n",
    "\n",
    "            ACC=rightPred/allsamps\n",
    "            print(\"epoch=\",epoch)\n",
    "            print('Accuracy is=',ACC*100)\n",
    "            #if the acc is greater than the best acc, save the model\n",
    "            if ACC>BestACC:\n",
    "                torch.save(model.state_dict(), './model/best_model.pth')\n",
    "                BestACC=ACC\n",
    "        model.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for MultiLayerFCNet:\n\tMissing key(s) in state_dict: \"layer1.weight\", \"layer1.bias\", \"B1.weight\", \"B1.bias\", \"B1.running_mean\", \"B1.running_var\", \"layer2.weight\", \"layer2.bias\", \"B2.weight\", \"B2.bias\", \"B2.running_mean\", \"B2.running_var\", \"layer3.weight\", \"layer3.bias\", \"B3.weight\", \"B3.bias\", \"B3.running_mean\", \"B3.running_var\", \"layer4.weight\", \"layer4.bias\", \"B4.weight\", \"B4.bias\", \"B4.running_mean\", \"B4.running_var\", \"layer5.weight\", \"layer5.bias\", \"B5.weight\", \"B5.bias\", \"B5.running_mean\", \"B5.running_var\", \"layer6.weight\", \"layer6.bias\", \"B6.weight\", \"B6.bias\", \"B6.running_mean\", \"B6.running_var\", \"layer7.weight\", \"layer7.bias\", \"B7.weight\", \"B7.bias\", \"B7.running_mean\", \"B7.running_var\", \"layer8.weight\", \"layer8.bias\", \"B8.weight\", \"B8.bias\", \"B8.running_mean\", \"B8.running_var\", \"fc.weight\", \"fc.bias\". \n\tUnexpected key(s) in state_dict: \"module.layer1.weight\", \"module.layer1.bias\", \"module.B1.weight\", \"module.B1.bias\", \"module.B1.running_mean\", \"module.B1.running_var\", \"module.B1.num_batches_tracked\", \"module.layer2.weight\", \"module.layer2.bias\", \"module.B2.weight\", \"module.B2.bias\", \"module.B2.running_mean\", \"module.B2.running_var\", \"module.B2.num_batches_tracked\", \"module.layer3.weight\", \"module.layer3.bias\", \"module.B3.weight\", \"module.B3.bias\", \"module.B3.running_mean\", \"module.B3.running_var\", \"module.B3.num_batches_tracked\", \"module.layer4.weight\", \"module.layer4.bias\", \"module.B4.weight\", \"module.B4.bias\", \"module.B4.running_mean\", \"module.B4.running_var\", \"module.B4.num_batches_tracked\", \"module.layer5.weight\", \"module.layer5.bias\", \"module.B5.weight\", \"module.B5.bias\", \"module.B5.running_mean\", \"module.B5.running_var\", \"module.B5.num_batches_tracked\", \"module.layer6.weight\", \"module.layer6.bias\", \"module.B6.weight\", \"module.B6.bias\", \"module.B6.running_mean\", \"module.B6.running_var\", \"module.B6.num_batches_tracked\", \"module.layer7.weight\", \"module.layer7.bias\", \"module.B7.weight\", \"module.B7.bias\", \"module.B7.running_mean\", \"module.B7.running_var\", \"module.B7.num_batches_tracked\", \"module.layer8.weight\", \"module.layer8.bias\", \"module.B8.weight\", \"module.B8.bias\", \"module.B8.running_mean\", \"module.B8.running_var\", \"module.B8.num_batches_tracked\", \"module.fc.weight\", \"module.fc.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/sayems_mac/ai-ducation-analytics/cnn_training.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sayems_mac/ai-ducation-analytics/cnn_training.ipynb#W3sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     model \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mDataParallel(model)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sayems_mac/ai-ducation-analytics/cnn_training.ipynb#W3sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# Load the model weights\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/sayems_mac/ai-ducation-analytics/cnn_training.ipynb#W3sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m model\u001b[39m.\u001b[39;49mload_state_dict(torch\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39m./model/best_model.pth\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sayems_mac/ai-ducation-analytics/cnn_training.ipynb#W3sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# Move the model to the appropriate device\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sayems_mac/ai-ducation-analytics/cnn_training.ipynb#W3sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:2152\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2147\u001b[0m         error_msgs\u001b[39m.\u001b[39minsert(\n\u001b[1;32m   2148\u001b[0m             \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMissing key(s) in state_dict: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   2149\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mk\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2151\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(error_msgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 2152\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   2153\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2154\u001b[0m \u001b[39mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for MultiLayerFCNet:\n\tMissing key(s) in state_dict: \"layer1.weight\", \"layer1.bias\", \"B1.weight\", \"B1.bias\", \"B1.running_mean\", \"B1.running_var\", \"layer2.weight\", \"layer2.bias\", \"B2.weight\", \"B2.bias\", \"B2.running_mean\", \"B2.running_var\", \"layer3.weight\", \"layer3.bias\", \"B3.weight\", \"B3.bias\", \"B3.running_mean\", \"B3.running_var\", \"layer4.weight\", \"layer4.bias\", \"B4.weight\", \"B4.bias\", \"B4.running_mean\", \"B4.running_var\", \"layer5.weight\", \"layer5.bias\", \"B5.weight\", \"B5.bias\", \"B5.running_mean\", \"B5.running_var\", \"layer6.weight\", \"layer6.bias\", \"B6.weight\", \"B6.bias\", \"B6.running_mean\", \"B6.running_var\", \"layer7.weight\", \"layer7.bias\", \"B7.weight\", \"B7.bias\", \"B7.running_mean\", \"B7.running_var\", \"layer8.weight\", \"layer8.bias\", \"B8.weight\", \"B8.bias\", \"B8.running_mean\", \"B8.running_var\", \"fc.weight\", \"fc.bias\". \n\tUnexpected key(s) in state_dict: \"module.layer1.weight\", \"module.layer1.bias\", \"module.B1.weight\", \"module.B1.bias\", \"module.B1.running_mean\", \"module.B1.running_var\", \"module.B1.num_batches_tracked\", \"module.layer2.weight\", \"module.layer2.bias\", \"module.B2.weight\", \"module.B2.bias\", \"module.B2.running_mean\", \"module.B2.running_var\", \"module.B2.num_batches_tracked\", \"module.layer3.weight\", \"module.layer3.bias\", \"module.B3.weight\", \"module.B3.bias\", \"module.B3.running_mean\", \"module.B3.running_var\", \"module.B3.num_batches_tracked\", \"module.layer4.weight\", \"module.layer4.bias\", \"module.B4.weight\", \"module.B4.bias\", \"module.B4.running_mean\", \"module.B4.running_var\", \"module.B4.num_batches_tracked\", \"module.layer5.weight\", \"module.layer5.bias\", \"module.B5.weight\", \"module.B5.bias\", \"module.B5.running_mean\", \"module.B5.running_var\", \"module.B5.num_batches_tracked\", \"module.layer6.weight\", \"module.layer6.bias\", \"module.B6.weight\", \"module.B6.bias\", \"module.B6.running_mean\", \"module.B6.running_var\", \"module.B6.num_batches_tracked\", \"module.layer7.weight\", \"module.layer7.bias\", \"module.B7.weight\", \"module.B7.bias\", \"module.B7.running_mean\", \"module.B7.running_var\", \"module.B7.num_batches_tracked\", \"module.layer8.weight\", \"module.layer8.bias\", \"module.B8.weight\", \"module.B8.bias\", \"module.B8.running_mean\", \"module.B8.running_var\", \"module.B8.num_batches_tracked\", \"module.fc.weight\", \"module.fc.bias\". "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 'MultiLayerFCNet' is the model class\n",
    "model = MultiLayerFCNet(input_size, hidden_size, output_size)\n",
    "\n",
    "\n",
    "# If the model was originally wrapped in nn.DataParallel for training, do the same here\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "# Load the model weights\n",
    "model.load_state_dict(torch.load('./model/best_model.pth'))\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "model = model.to(device)\n",
    "\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "def custom_loader(batch_size, shuffle_test=False, data_path='./dataset_split'):\n",
    "    # Add the necessary transforms\n",
    "    # normalize = transforms.Normalize(mean=[0.024], std=[0.994])\n",
    "    # transform = transforms.Compose([\n",
    "    #     transforms.Resize((48, 48)),  # Adjust this if your images are a different size\n",
    "    #     # transforms.Grayscale(num_output_channels=1),  # Convert to grayscale\n",
    "    #     transforms.ToTensor(),\n",
    "    #     # normalize\n",
    "    # ])\n",
    "\n",
    "    # Load your custom dataset\n",
    "    test_dataset = datasets.ImageFolder(root=data_path + '/Test')\n",
    "\n",
    "    # Data loaders\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=shuffle_test, pin_memory=True)\n",
    "\n",
    "    return  test_loader\n",
    "\n",
    "\n",
    "# Prepare your test loader\n",
    "test_loader = custom_loader(batch_size, shuffle_test=True, data_path='./dataset_split')\n",
    "\n",
    "# Initialize lists to track predictions and true labels\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "# No gradients needed for evaluation\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Get predictions\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        # Store predictions and true labels\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap='Blues')\n",
    "ax.set_xlabel('Predicted Labels')\n",
    "ax.set_ylabel('True Labels')\n",
    "ax.set_title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
