{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.100479366732579\n",
      "epoch= 0\n",
      "Accuracy is= tensor(39.6491)\n",
      "1.3340063562580184\n",
      "epoch= 1\n",
      "Accuracy is= tensor(44.0351)\n",
      "1.1675117869003147\n",
      "epoch= 2\n",
      "Accuracy is= tensor(45.4386)\n",
      "1.0721565344754387\n",
      "epoch= 3\n",
      "Accuracy is= tensor(49.4737)\n",
      "1.0389226219233345\n",
      "epoch= 4\n",
      "Accuracy is= tensor(50.)\n",
      "0.9315179376041188\n",
      "epoch= 5\n",
      "Accuracy is= tensor(54.7368)\n",
      "0.8684279626491023\n",
      "epoch= 6\n",
      "Accuracy is= tensor(54.9123)\n",
      "0.8186756559446746\n",
      "epoch= 7\n",
      "Accuracy is= tensor(58.4211)\n",
      "0.7531094971825095\n",
      "epoch= 8\n",
      "Accuracy is= tensor(57.5439)\n",
      "0.7188600348491295\n",
      "epoch= 9\n",
      "Accuracy is= tensor(55.2632)\n",
      "0.7086352302747614\n",
      "epoch= 10\n",
      "Accuracy is= tensor(58.5965)\n",
      "0.6225613811436821\n",
      "epoch= 11\n",
      "Accuracy is= tensor(55.9649)\n",
      "0.5825706276239133\n",
      "epoch= 12\n",
      "Accuracy is= tensor(61.2281)\n",
      "0.5386465799574759\n",
      "epoch= 13\n",
      "Accuracy is= tensor(61.9298)\n",
      "0.5353217288559559\n",
      "epoch= 14\n",
      "Accuracy is= tensor(57.1930)\n",
      "0.4446299677970363\n",
      "epoch= 15\n",
      "Accuracy is= tensor(63.3333)\n",
      "0.4075356911794812\n",
      "epoch= 16\n",
      "Accuracy is= tensor(64.5614)\n",
      "0.41992168274580266\n",
      "epoch= 17\n",
      "Accuracy is= tensor(59.4737)\n",
      "0.3598122047443016\n",
      "epoch= 18\n",
      "Accuracy is= tensor(59.4737)\n",
      "0.34963006073353337\n",
      "epoch= 19\n",
      "Accuracy is= tensor(63.8596)\n",
      "0.2920311332917681\n",
      "epoch= 20\n",
      "Accuracy is= tensor(61.2281)\n",
      "0.29352161872620675\n",
      "epoch= 21\n",
      "Accuracy is= tensor(61.4035)\n",
      "0.253345306597504\n",
      "epoch= 22\n",
      "Accuracy is= tensor(58.9474)\n",
      "0.22008596974260666\n",
      "epoch= 23\n",
      "Accuracy is= tensor(59.8246)\n",
      "0.26941956345941503\n",
      "epoch= 24\n",
      "Accuracy is= tensor(60.7018)\n",
      "0.20814705698513516\n",
      "epoch= 25\n",
      "Accuracy is= tensor(61.7544)\n",
      "0.17483681433048903\n",
      "epoch= 26\n",
      "Accuracy is= tensor(62.1053)\n",
      "0.17924020782697433\n",
      "epoch= 27\n",
      "Accuracy is= tensor(60.1754)\n",
      "0.16420499432612867\n",
      "epoch= 28\n",
      "Accuracy is= tensor(61.7544)\n",
      "0.1698278862544719\n",
      "epoch= 29\n",
      "Accuracy is= tensor(59.2982)\n",
      "0.1485697543065922\n",
      "epoch= 30\n",
      "Accuracy is= tensor(62.1053)\n",
      "0.15844364624981785\n",
      "epoch= 31\n",
      "Accuracy is= tensor(61.7544)\n",
      "0.1595614423938826\n",
      "epoch= 32\n",
      "Accuracy is= tensor(60.8772)\n",
      "0.13470217344515464\n",
      "epoch= 33\n",
      "Accuracy is= tensor(61.5789)\n",
      "0.14936931807474763\n",
      "epoch= 34\n",
      "Accuracy is= tensor(60.8772)\n",
      "0.15143009939906643\n",
      "epoch= 35\n",
      "Accuracy is= tensor(62.1053)\n",
      "0.13803793219666854\n",
      "epoch= 36\n",
      "Accuracy is= tensor(59.8246)\n",
      "0.14270619503861548\n",
      "epoch= 37\n",
      "Accuracy is= tensor(60.3509)\n",
      "0.1564323603376454\n",
      "epoch= 38\n",
      "Accuracy is= tensor(62.6316)\n",
      "0.1361436328175021\n",
      "epoch= 39\n",
      "Accuracy is= tensor(61.5789)\n",
      "0.1373144032104927\n",
      "epoch= 40\n",
      "Accuracy is= tensor(60.0000)\n",
      "0.11971714683607512\n",
      "epoch= 41\n",
      "Accuracy is= tensor(61.9298)\n",
      "0.13459208304537276\n",
      "epoch= 42\n",
      "Accuracy is= tensor(59.2982)\n",
      "0.1322065087071821\n",
      "epoch= 43\n",
      "Accuracy is= tensor(60.5263)\n",
      "0.12143795740078478\n",
      "epoch= 44\n",
      "Accuracy is= tensor(61.9298)\n",
      "0.13723789940716005\n",
      "epoch= 45\n",
      "Accuracy is= tensor(62.1053)\n",
      "0.13241662727851494\n",
      "epoch= 46\n",
      "Accuracy is= tensor(61.7544)\n",
      "0.12510299938274363\n",
      "epoch= 47\n",
      "Accuracy is= tensor(59.2982)\n",
      "0.1415131327641361\n",
      "epoch= 48\n",
      "Accuracy is= tensor(60.7018)\n",
      "0.12559203945976846\n",
      "epoch= 49\n",
      "Accuracy is= tensor(60.7018)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torch import optim, nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import random_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as td\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "\n",
    "def custom_loader(batch_size, shuffle_test=False, data_path='./Dataset/Train'):\n",
    "    # Add the necessary transforms\n",
    "    # normalize = transforms.Normalize(mean=[0.024], std=[0.994])\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((48, 48)),  # Adjust this if your images are a different size\n",
    "        # transforms.Grayscale(num_output_channels=1),  # Convert to grayscale\n",
    "        transforms.ToTensor(),\n",
    "        # normalize\n",
    "    ])\n",
    "\n",
    "    # Load your dataset using ImageFolder\n",
    "    master_dataset = datasets.ImageFolder(root=data_path, transform=transform)\n",
    "\n",
    "    # Calculate the sizes of the splits\n",
    "    total_size = len(master_dataset)\n",
    "    train_size = int(0.85 * total_size)\n",
    "    val_size = total_size - train_size\n",
    "    # print(val_size)\n",
    "    # print(train_size)\n",
    "\n",
    "    # Use random_split to create datasets for training, testing, and validation\n",
    "    train_dataset, val_dataset = random_split(master_dataset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "\n",
    "class MultiLayerFCNet(nn.Module):\n",
    "    def __init__(self,input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # we have a 4 layer network\n",
    "        # 1. input layer\n",
    "        # we are using stride 1 to reduce the down sampling of the image\n",
    "        # we are using padding 1 to make sure the image size is same after convolution\n",
    "        self.layer1=nn.Conv2d(3,32,3,padding=1,stride=1)\n",
    "        # we are using batch normalization to normalize the output of the convolution layer , it mitigates the problem of exploding/vanishing gradients,\n",
    "        #  mitigates the problem of exploding/vanishing gradients\n",
    "        #  and also helps the model to converge faster\n",
    "        self.B1 = nn.BatchNorm2d(32)\n",
    "        # 2. hidden layer\n",
    "        # input size is 32 because the output of the previous layer is 32\n",
    "        self.layer2 = nn.Conv2d(32, 32, 3, padding=1, stride=1)\n",
    "        # again we are using batch normalization\n",
    "        self.B2 = nn.BatchNorm2d(32)\n",
    "        # we are using max pooling to reduce the size of the image\n",
    "        self.Maxpool=nn.MaxPool2d(2)\n",
    "        # 3. hidden layer\n",
    "        # input size is 32 because the output of the previous layer is 32\n",
    "        # new hidden layer is 64\n",
    "        self.layer3 = nn.Conv2d(32, 64, 3, padding=1, stride=1)\n",
    "        # again we are using batch normalization\n",
    "        self.B3 = nn.BatchNorm2d(64)\n",
    "        # 4. hidden layer\n",
    "        # input size is 64 because the output of the previous layer is 64\n",
    "        self.layer4 = nn.Conv2d(64, 64, 3, padding=1, stride=1)\n",
    "        # again we are using batch normalization\n",
    "        self.B4 = nn.BatchNorm2d(64)\n",
    "        # we are using dropout to reduce the overfitting of the model\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        # we are using fully connected layer to get the output\n",
    "        # reduced the size of the image to 12*12 beacuse of the max pooling\n",
    "        self.fc_size = 64 * 12 * 12  \n",
    "        self.fc = nn.Linear(self.fc_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through existing layers\n",
    "        # from the input layer to the first hidden layer we are using leaky relu as activation function\n",
    "        # we are using leaky relu because it mitigates the problem of dying relu\n",
    "        x = F.leaky_relu(self.B1(self.layer1(x)))\n",
    "        # we are using max pooling to reduce the size of the image on the first hidden layer\n",
    "        x = self.Maxpool(F.leaky_relu(self.B2(self.layer2(x))))\n",
    "        # we using leaky relu as activation function on the second hidden layer\n",
    "        x = F.leaky_relu(self.B3(self.layer3(x)))\n",
    "        # we are using max pooling to reduce the size of the image on the second hidden layer\n",
    "        x = self.Maxpool(F.leaky_relu(self.B4(self.layer4(x))))\n",
    "\n",
    "        # Flatten the tensor for the fully connected layer\n",
    "        x = x.view(x.size(0), -1) \n",
    "        return self.fc(x)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    batch_size = 64\n",
    "    test_batch_size = 64\n",
    "    input_size = 3 * 48 * 48  # 3 channels, 48x48 image size\n",
    "    hidden_size = 50  # Number of hidden units\n",
    "    output_size = 4  # Number of output classes 4\n",
    "    num_epochs = 10\n",
    "\n",
    "    # train_loader, _ = cifar_loader(batch_size)\n",
    "    # _, test_loader = cifar_loader(test_batch_size)\n",
    "    train_loader, test_loader = custom_loader(batch_size, data_path='./dataset/Train')\n",
    "    # dataloader = DataLoader(dataset=IrisDataset('iris.data'),\n",
    "    #                         batch_size=10,\n",
    "    #                         shuffle=True)\n",
    "\n",
    "    epochs = 50\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = MultiLayerFCNet(input_size, hidden_size, output_size)\n",
    "    model = nn.DataParallel(model)\n",
    "    model.to(device)\n",
    "    #model.load_state_dict(torch.load('path'))\n",
    "\n",
    "    # Loss and optimizer\n",
    "    #Cross-Entropy Loss measures the performance of the classification model whose output is a probability value between 0 and 1.\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    #The learning rate controls how much the model's weights should be adjusted with respect to the loss gradient.\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    BestACC=0.3\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0\n",
    "        for instances, labels in train_loader:\n",
    "            optimizer.zero_grad() # Reset gradients to zero for each instance\n",
    "\n",
    "            output = model(instances) # Forward pass\n",
    "            loss = criterion(output, labels) # Compute loss\n",
    "            loss.backward() # Backward pass (compute gradients)\n",
    "            optimizer.step() # Update weights\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(running_loss / len(train_loader))\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            allsamps=0\n",
    "            rightPred=0\n",
    "\n",
    "            for instances, labels in test_loader:\n",
    "\n",
    "                # Feed the instances (input data) through the model to get predictions\n",
    "                # 'output' contains the raw scores for each class, outputted by the model\n",
    "\n",
    "                output = model(instances)\n",
    "\n",
    "                # 'torch.max(output, 1)' finds the maximum value along dimension 1 (class dimension)\n",
    "                # This returns the predicted class for each instance in the batch\n",
    "                # 'predictedClass' will contain two tensors: one with the max values, another with their corresponding indices (class labels\n",
    "\n",
    "                predictedClass=torch.max(output,1)\n",
    "                allsamps+=output.size(0) # sum up all the instances\n",
    "                rightPred+=(torch.max(output,1)[1]==labels).sum() # sum up all the correct predictions\n",
    "\n",
    "\n",
    "\n",
    "            ACC=rightPred/allsamps # calculate the accuracy\n",
    "            print(\"epoch=\",epoch)\n",
    "            print('Accuracy is=',ACC*100)\n",
    "            #if the acc is greater than the best acc, save the model\n",
    "            \n",
    "            if ACC>BestACC:\n",
    "                torch.save(model.state_dict(), './model/model_variant1.pth')\n",
    "                BestACC=ACC\n",
    "\n",
    "        model.train()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
